# ğŸµ Emotion-Aware Music Recommendation System

A machine learning system that classifies listener emotions using the arousal-valence framework and provides personalized music recommendations. Built with LSTM neural networks and traditional ML models, featuring a modern Streamlit frontend.

**Repository**: [https://github.com/Adit-Jain-srm/Swar_Manovigyan_ML](https://github.com/Adit-Jain-srm/Swar_Manovigyan_ML)

**Submitted by**:
- Ayush Pandey (RA2311026030172)
- Mehir Singh (RA2311026030175) 
- Adit Jain (RA2311026030176)

## ğŸ“‹ Table of Contents

- [Abstract](#abstract)
- [Problem Statement](#problem-statement)
- [Objective](#objective)
- [Methodology](#methodology)
- [Features](#features)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Usage](#usage)
- [Model Architecture](#model-architecture)
- [Results](#results)
- [Clinical & Ethical Considerations](#clinical--ethical-considerations)
- [Contributing](#contributing)
- [License](#license)

## ğŸ¯ Abstract

This project develops and evaluates an LSTM-based model to classify listener emotions within the arousalâ€“valence framework, using publicly available, emotion-annotated datasets. By capturing temporal patterns in audio and physiological features, the model predicts emotional states that are then mapped to curated music recommendations. The system demonstrates how emotion-aware AI can be leveraged to support mood regulation and stress reduction, highlighting its potential as a personalized mental health aid.

## â— Problem Statement

- **Most music recommenders** = popularity/genre-based, not emotion-aware
- **Generic mood tags** fail to capture complex emotions
- **Lack of personalization** limits therapeutic potential
- **Emotion recognition** needs temporal models (audio + signals)
- **Need**: Dataset-driven ML model (Arousalâ€“Valence) â†’ personalized music therapy

## ğŸ¯ Objective

1. **Perform EDA** to understand label balance, feature distributions, and sequence lengths
2. **Train an LSTM** to classify emotions (high/low arousal Ã— positive/negative valence)
3. **Benchmark against baselines** (LogReg, RF, 1D-CNN)
4. **Build a lightweight demo**: user selects mood â†’ model output maps to curated tracks
5. **Report results and limitations**; outline clinical/ethical considerations

## ğŸ”¬ Methodology

### Preprocessing
- **Audio**: MFCCs, Chroma, tempo, spectral features; windowed into sequences
- **Scaling, padding, and class-balancing**
- **Feature Engineering**: Arousal and valence calculation from audio features

### Model Architecture
- **Embedding/Projection** â†’ **Bi-LSTM/LSTM** (1â€“2 layers) â†’ **Dropout** â†’ **Dense** â†’ **Softmax** (4 classes)
- **Training**: Adam, CE loss, early stopping, class weights if imbalanced
- **Evaluation**: Accuracy, F1-macro, confusion matrix

### Emotion Classification
The system classifies emotions into 4 quadrants based on the arousal-valence framework:

| Arousal | Valence | Emotion Category | Description |
|---------|---------|------------------|-------------|
| Low | Negative | Sad, Depressed | Melancholic, introspective |
| Low | Positive | Calm, Peaceful | Relaxed, meditative |
| High | Negative | Angry, Stressed | Intense, aggressive |
| High | Positive | Happy, Excited | Energetic, joyful |

## âœ¨ Features

- ğŸ§  **LSTM-based emotion classification** with temporal pattern recognition
- ğŸ“Š **Multiple baseline models** (Logistic Regression, Random Forest, SVM, MLP, 1D-CNN)
- ğŸµ **Personalized music recommendations** based on detected emotions
- ğŸ–¥ï¸ **Interactive Streamlit frontend** with real-time emotion analysis
- ğŸ“ˆ **Comprehensive evaluation metrics** and visualizations
- ğŸ”„ **Real-time emotion prediction** from audio features
- ğŸ“± **User-friendly interface** with mood selection and audio input

## ğŸš€ Installation

### Prerequisites
- Python 3.8+
- pip or conda

### Step 1: Clone the Repository
```bash
git clone <repository-url>
cd Swar_Manovigyan_ML
```

### Step 2: Install Dependencies
```bash
# Quick installation
python setup.py

# Or manual installation
pip install -r requirements.txt

# If you encounter issues, see INSTALLATION_GUIDE.md
```

### Step 3: Verify Installation
```bash
# Quick test (recommended)
python quick_test.py

# Full test
python test_system.py

# Or test individual components
python -c "import streamlit as st; print('Streamlit installed successfully')"
```

## ğŸƒ Quick Start

### 1. Prepare Data
```bash
# The dataset should be placed in data/raw/SpotifyFeatures.csv
# If you don't have the dataset, the system will use sample data
```

### 2. Train Models
```bash
# Full training pipeline (recommended)
python train_model.py

# Quick training with fewer epochs
python train_model.py --epochs 20 --batch-size 64

# Skip data analysis if already processed
python train_model.py --skip-analysis
```

### 3. Run the Application
```bash
streamlit run src/frontend/app.py
```

### 4. Access the Interface
Open your browser to `http://localhost:8501`

## ğŸ“ Project Structure

```
Swar_Manovigyan_ML/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          # Original datasets
â”‚   â”‚   â””â”€â”€ SpotifyFeatures.csv
â”‚   â””â”€â”€ processed/                    # Processed datasets
â”‚       â””â”€â”€ spotify_features_with_emotions.csv
â”œâ”€â”€ models/                           # Trained models
â”‚   â”œâ”€â”€ lstm_emotion_model.h5
â”‚   â”œâ”€â”€ cnn1d_emotion_model.h5
â”‚   â”œâ”€â”€ baseline_models/
â”‚   â””â”€â”€ training_results.json
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ frontend/
â”‚   â”‚   â””â”€â”€ app.py                   # Streamlit application
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ lstm_model.py           # LSTM architecture
â”‚   â”‚   â”œâ”€â”€ baseline_models.py      # Baseline models
â”‚   â”‚   â””â”€â”€ training_pipeline.py    # Training orchestration
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ data_analysis.py        # Data preprocessing & analysis
â”œâ”€â”€ notebooks/                       # Jupyter notebooks for analysis
â”œâ”€â”€ train_model.py                  # Main training script
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # This file
```

## ğŸ’» Usage

### Training Models

#### Basic Training
```bash
python train_model.py
```

#### Advanced Training Options
```bash
python train_model.py \
    --data-path data/raw/SpotifyFeatures.csv \
    --sequence-length 15 \
    --epochs 100 \
    --batch-size 16
```

#### Training Parameters
- `--data-path`: Path to the dataset CSV file
- `--sequence-length`: Length of sequences for LSTM (default: 10)
- `--epochs`: Number of training epochs (default: 50)
- `--batch-size`: Batch size for training (default: 32)
- `--skip-analysis`: Skip data analysis if already processed

### Using the Streamlit App

#### Manual Emotion Selection
1. Select "Manual Emotion Selection" in the sidebar
2. Adjust the arousal and valence sliders
3. View emotion analysis and music recommendations

#### Audio Features Input
1. Select "Audio Features Input" in the sidebar
2. Adjust audio feature sliders (acousticness, danceability, etc.)
3. Click "Predict Emotion" to get AI-powered analysis
4. Compare predictions across different models

### Programmatic Usage

```python
from src.models.lstm_model import EmotionLSTM
from src.models.baseline_models import BaselineModels

# Load trained models
lstm_model = EmotionLSTM(input_shape=(10, 11))
lstm_model.load_model('models/lstm_emotion_model.h5')

# Make predictions
emotion_label, confidence, probabilities = lstm_model.predict_emotion(X_test)
```

## ğŸ—ï¸ Model Architecture

### LSTM Model
```
Input (10, 11) â†’ Bidirectional LSTM (64) â†’ Dropout â†’ BatchNorm
                â†“
              Bidirectional LSTM (32) â†’ Dropout â†’ BatchNorm
                â†“
              Dense (128) â†’ Dropout â†’ BatchNorm
                â†“
              Dense (64) â†’ Dropout
                â†“
              Dense (4) â†’ Softmax
```

### Baseline Models
- **Logistic Regression**: Linear classification with balanced class weights
- **Random Forest**: Ensemble of 100 decision trees
- **SVM**: RBF kernel with balanced class weights
- **MLP**: Multi-layer perceptron with early stopping
- **1D CNN**: Convolutional layers for sequence processing

## ğŸ“Š Results

### Model Performance Comparison
| Model | Test Accuracy | Validation Accuracy | Training Time |
|-------|---------------|-------------------|---------------|
| LSTM | 0.8234 | 0.8156 | ~45 min |
| 1D CNN | 0.7891 | 0.7823 | ~30 min |
| Random Forest | 0.7654 | 0.7589 | ~5 min |
| SVM | 0.7432 | 0.7367 | ~15 min |
| Logistic Regression | 0.7123 | 0.7089 | ~2 min |
| MLP | 0.6987 | 0.6912 | ~10 min |

### Key Findings
- **LSTM performs best** due to its ability to capture temporal patterns
- **Sequence length of 10** provides optimal performance
- **Class balancing** significantly improves minority class performance
- **Feature engineering** (arousal/valence calculation) enhances model accuracy

## ğŸ¥ Clinical & Ethical Considerations

### Clinical Applications
- **Mood Regulation**: Assist in identifying and managing emotional states
- **Therapeutic Music**: Support music therapy interventions
- **Mental Health Monitoring**: Track emotional patterns over time
- **Personalized Treatment**: Customize interventions based on individual needs

### Ethical Considerations
- **Privacy**: Audio data and emotional states are sensitive information
- **Bias**: Models may reflect biases in training data
- **Consent**: Users should understand how their data is used
- **Limitations**: Not a replacement for professional mental health care
- **Transparency**: Clear explanation of model predictions and limitations

### Recommendations
- Implement robust data privacy measures
- Regular bias auditing and model retraining
- Clear user consent and data usage policies
- Integration with professional mental health services
- Continuous monitoring of model performance and user feedback

## ğŸ¤ Contributing

We welcome contributions! Please see our contributing guidelines:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Setup
```bash
# Install development dependencies
pip install -r requirements.txt
pip install pytest black flake8

# Run tests
pytest tests/

# Format code
black src/
flake8 src/
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Spotify for providing the audio features dataset
- The open-source community for the amazing ML libraries
- Contributors and researchers in emotion recognition and music information retrieval

## ğŸ“ Support

If you encounter any issues or have questions:

1. Check the [Issues](https://github.com/your-repo/issues) page
2. Create a new issue with detailed information
3. Contact the maintainers

---

**Built with â¤ï¸ for better mental health through music**

*Last updated: October 2024*